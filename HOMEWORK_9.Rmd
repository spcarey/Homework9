---
title: "HOMEWORK 9"
author: "Sean  Carey"
date: "11/8/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tree)
library(randomForest)
library(gbm)
set.seed(2341435)

```

## 1.

```{r 1.}

#read in data
AMZ <- read_csv("https://www.openml.org/data/get_csv/1681098/phpmPOD5A")
#set "Target" as factor
AMZ$target <- as.factor(AMZ$target)

#split AMZ into training and test data sets

train <- sample(1:nrow(AMZ), size = nrow(AMZ)*0.8)
test <- dplyr::setdiff(1:nrow(AMZ), train)

#create training and test data sets
AMZ_training <-  AMZ[train, ]
AMZ_test <- AMZ[test, ]



```
##2.  

###Classification Tree Fit
```{r train class model, cache=TRUE}

#Fit Regression Tree 

tree_fit_1 <- tree(target ~ ., data = AMZ_training)
#Tree only has one node!!!
tree_pred <- predict(tree_fit_1, type = "class", newdata = dplyr::select(AMZ_test, -target))


tree_table <- table(tree_pred, AMZ_test$target)

tree_table

tree_test_error <- sum(diag(tree_table)/sum(tree_table))

tree_test_error

```

### Bagging

```{r train bagged Tree Model, cache=TRUE}

#use bagging for tree model

bag_fit_1 <- randomForest(target ~ ., data = AMZ_training, mtry = ncol(AMZ_training)-1, ntree = 500, importance=TRUE )

bag_pred <- predict(bag_fit_1, newdata = dplyr::select(AMZ_test, -target), n.trees=500)


bag_table <- table(bag_pred, AMZ_test$target)

bag_table

bag_test_error <- sum(diag(bag_table)/sum(bag_table))

bag_test_error

```

### Random Forest
```{r Rand Forest Fit, cache=TRUE}




rf_fit_1 <- randomForest(target ~ ., data = AMZ_training, mtry = (ncol(AMZ_training)-1)/3, ntree = 500, importance=TRUE )



rf_pred <- predict(rf_fit_1, newdata = dplyr::select(AMZ_test, -target), n.trees=500)



rf_table <- table(rf_pred, AMZ_test$target)

rf_table

rf_test_error <- sum(diag(rf_table)/sum(rf_table))

rf_test_error

```

```{r boosted fit, cache=TRUE}


#model didn't work using target as a factor. It also didn'like numeric. Used character
boost_fit_1 <- gbm(as.numeric(as.character(target)) ~ ., data = AMZ_training, distribution = "bernoulli",cv.folds = 5 , n.trees = 3500, shrinkage = 0.1, interaction.depth = 3)


boost_fit_1



#obtain the best cross validated # of trees
boost_fit_1_best_cv <- gbm.perf(boost_fit_1, method = "cv")

boost_fit_1_best_cv

#make prediction based on best Cross Validation number of trees and best test number of trees
boost_pred <- predict.gbm(boost_fit_1, newdata = dplyr::select(AMZ_test, -target),type = "response", n.trees = boost_fit_1_best_cv)


#the fit gives the log odds of the resonse instead of the logical 0,1
#convert boost pred to a tibble
boost_pred <- boost_pred %>% as.tibble



#add a column of the predicted values from each number of trees
#use the log odds to say what group the fit will belong to
boost_pred$class_cv <- ifelse(boost_pred$value >= 0.5, 1, 0)


#create table comparing test data to predictons using cross validated # of trees
boost_table_cv <- table(data.frame(boost_pred$class_cv, AMZ_test$target))


#obtain test error rate of cross validated number of trees
boost_test_error_cv <- sum(diag(boost_table_cv)/sum(boost_table_cv))

boost_test_error_cv


```
### Model Comparisons
```{r}

ModelCompare <- c("Tree Model"=tree_test_error, "Bagged Model"=bag_test_error, "Random Forest"= rf_test_error, "Boosting Test Error"=boost_test_error_cv)

knitr::kable(ModelCompare)

```

